{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WebScrapping-ElPais_01.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zezake/Web_Scraping/blob/master/WebScrapping_ElPais_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "yHmY1RuaWQhY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Codi del Dataset\n",
        "\n",
        "\n",
        "> Aleix Martinez i Ignasi Vilarasau\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "XrBJV8EOa5sA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Importem les llibreries necessaries per executar el codi\"\"\"\n",
        "\n",
        "#Webscrapping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "#Per incloure una columna amb el moment que s'ha fet l'extracció\n",
        "from datetime import datetime\n",
        "#Extracció de fitxer del Colab\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k6_RYyYhf8BY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"  Neteja del text\n",
        "Extreiem els salts de pàgina que poden apareixer dins dels diferents camps.\n",
        "A més a més extreiem els signes de puntuació que poden interferir amb el format del fitxer csv\n",
        "\"\"\"\n",
        "def neteja_text(text_html):\n",
        "  try:\n",
        "    text_html = text_html.text.strip().replace('\\n','')\n",
        "    text_html = treure_puntuacio(text_html)\n",
        "  except:\n",
        "    text_html = 'NA'\n",
        "  \n",
        "  return text_html\n",
        "\n",
        "def treure_puntuacio(text):\n",
        "  puntuacio = [',',';']\n",
        "  for p in puntuacio:\n",
        "    text = text.replace(p,'')\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DZW0A77IqL5b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Funcions per extreure el tipus i subtipus de cada secció\n",
        "\"\"\"\n",
        "def get_section_subtype(section):\n",
        "  section_name = section.find('a',{'class':'enlace'})\n",
        "  return neteja_text(section_name)\n",
        "  \n",
        "\n",
        "def get_section_type(section):\n",
        "  section_type = section.find('h3')\n",
        "  return neteja_text(section_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rXso-t4R4w35",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Funcions per extreure la informació bàsica de cada article.\n",
        "Aquestes són l'autor, el titol, la localització i el número de comentaris.\n",
        "\"\"\"\n",
        "\n",
        "def get_article_author(article):\n",
        "  article_author = article.find('span',{'itemprop':'name'})\n",
        "  #article_author = article.find('div', {'class':'firma'})\n",
        "  return neteja_text(article_author)\n",
        "\n",
        "def get_article_title(article):\n",
        "  article_title = article.find('h2')\n",
        "  return neteja_text(article_title)\n",
        "\n",
        "def get_article_location(article):\n",
        "  article_location = article.find('span',{'class':'articulo-localizacion'})\n",
        "  return neteja_text(article_location)\n",
        "\n",
        "def get_article_comments(article):\n",
        "  article_comments = article.find('span',{'class':'articulo-comentarios'})\n",
        "  return neteja_text(article_comments)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hUVkaHaCFGiJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Funcions per extreure la informacio de les fotos dels articles.\n",
        "Consisteix en extreure el peu de foto i l'autor de la imatge.\n",
        "\"\"\"\n",
        "\n",
        "def get_article_foto_text(article):\n",
        "  article_foto_text = article.find('span',{'class':'foto-texto'})\n",
        "  return neteja_text(article_foto_text)\n",
        "\n",
        "def get_article_foto_author(article):\n",
        "  article_foto_author = article.find('span',{'class':'foto-firma'})\n",
        "  return neteja_text(article_foto_author)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fa83UdB2fobF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Funció especifica per extreure la data de creació de la noticia.\n",
        "La data es retornada en el format seguent: YYYY-MM-DDThh:mm:ss\n",
        "D'ella extreiem el dia-mes-any i l'hora de creació de l'article.\n",
        "\"\"\"\n",
        "def get_article_date(article):\n",
        "  article_data = article.findAll('meta')\n",
        "  date = [_.get('content', None) for _ in article_data][-2]\n",
        "  hour, new_date = extract_date(date)\n",
        "  return hour, new_date\n",
        "\n",
        "def extract_date(date):\n",
        "  year = date[:4]\n",
        "  month = date[5:7]\n",
        "  day = date[8:10]\n",
        "  new_date = '-'.join([day,month,year])\n",
        "  hour = date[11:13]\n",
        "  return hour, new_date"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gfcw8T5kyCAd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Funció que serveix per cridar les funcions anteriorment descrites.\n",
        "L'objectiu és crear una llista que agrupi tota la informació \n",
        "\"\"\"\n",
        "\n",
        "def data_from_article(article):\n",
        "  a_title = get_article_title(a)\n",
        "  a_author = get_article_author(a)\n",
        "  a_location = get_article_location(a)\n",
        "  a_comments = get_article_comments(a)\n",
        "  a_hour, a_date = get_article_date(a)\n",
        "  a_foto_text = get_article_foto_text(a)\n",
        "  a_foto_author = get_article_foto_author(a)\n",
        "  a_extraction_date = datetime.today().strftime('%d-%m-%Y')\n",
        "  a_extraction_hour = datetime.today().strftime('%H')\n",
        "  Data = [a_extraction_date, a_extraction_hour, a_date, a_hour,\n",
        "         a_title, a_author, a_location,\n",
        "         a_comments, a_foto_author, a_foto_text]\n",
        "  \n",
        "  \n",
        "  return Data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E2U4J6QNbWip",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Extreiem l'hora en la qual es farà el webscrapping\n",
        "today = datetime.today().strftime('%d-%m-%Y')\n",
        "hour = datetime.today().strftime('%H')\n",
        "\n",
        "#Generem un nom del fitxer amb la seguent regla: (nom del dataset)_(hora extracció)_(data_extracció).txt\n",
        "filename = 'dataset_elpais_'+hour+'_'+today+'.txt'\n",
        "\n",
        "#Extreiem el HTML i fem el parsing amb bs4\n",
        "url = \"https://www.elpais.com\"\n",
        "html = requests.get(url)\n",
        "codi_html = html.content\n",
        "soup = BeautifulSoup(codi_html, 'html.parser')\n",
        "\n",
        "#Busquem les diferents seccions del diari\n",
        "sections = soup.findAll('section', limit = None)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B61cWHl256Jf",
        "colab_type": "code",
        "outputId": "2bec5651-6276-43f3-bdc1-608a92c3adc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "cell_type": "code",
      "source": [
        "#Obrim el fitxer\n",
        "with open(filename,'w') as f:\n",
        "  #Escribim el header on indiquem cada columna del dataset separada per comes, fitxer CSV.\n",
        "  f.write(\"\"\"Extraction Date,Extraction Hour,Publication Date,Publication Hour,Title,Author,Location,Num Comments,Photo Author,Photo Text,Sectiom,Subsection\\n\"\"\")\n",
        "\n",
        "  #Recorrem iterativament sobre totes les seccions del diari\n",
        "  for i, s in enumerate(sections):\n",
        "    #Extreiem informació respecte la secció\n",
        "    s_type = get_section_type(s)\n",
        "    s_subtype = get_section_subtype(s)\n",
        "    \n",
        "    #Informem de quina secció es procedeix a llegir\n",
        "    print('{} \\t  Secció: {}'.format(i, s_type))\n",
        "    \n",
        "    #Busquem els articles de cada secció i iterem sobre cadascún d'ells\n",
        "    articles = s.findAll('article')\n",
        "    for j, a in enumerate(articles):\n",
        "      \n",
        "      #Etreiem tota la informació de l'article en una llista\n",
        "      data = data_from_article(a)\n",
        "      #Afegim la secció a la qual pertany l'article\n",
        "      data.append(s_type)\n",
        "      data.append(s_subtype)\n",
        "      #Convertim els elements de la llista en un array separant cada columna per comes\n",
        "      data_to_write = ','.join(data)\n",
        "      \n",
        "      #Escribim la informació al fitxer\n",
        "      f.write(data_to_write+'\\n')\n",
        "      \n",
        "#Descarregem el fitxer.\n",
        "files.download(filename)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 \t  Secció: NA\n",
            "1 \t  Secció: Destacados\n",
            "2 \t  Secció: opinión\n",
            "3 \t  Secció: AMÉRICAS\n",
            "4 \t  Secció: INTERNACIONAL\n",
            "5 \t  Secció: ¿Y tú qué piensas?\n",
            "6 \t  Secció: El País Economía\n",
            "7 \t  Secció: Sociedad\n",
            "8 \t  Secció: Cultura\n",
            "9 \t  Secció: Deportes\n",
            "10 \t  Secció: ESPAÑA\n",
            "11 \t  Secció: Verne\n",
            "12 \t  Secció: Ciencia y Tecnología\n",
            "13 \t  Secció: Estilo y Vida\n",
            "14 \t  Secció: UN PAÍS DE BLOGS\n",
            "15 \t  Secció: VÍDEOS Y FOTOS\n",
            "16 \t  Secció: DESDE EL PAÍS\n",
            "17 \t  Secció: ONE\n",
            "18 \t  Secció: Entrevista W\n",
            "19 \t  Secció: TERMÓMETRO ECONÓMICO Y SOCIAL DE AMÉRICA\n",
            "20 \t  Secció: EL PAÍS IN ENGLISH\n",
            "21 \t  Secció: Lo más visto\n",
            "22 \t  Secció: NA\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}